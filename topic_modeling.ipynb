{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Topic Modeling\n",
    "\n",
    "Testing topic modeling on a tripadvisor hotel reviews dataset\n",
    "\n",
    "1. Load the dataset and preprocess the reviews\n",
    "2. Perform Topic Modeling using two different libraries:\n",
    "    1. sklearn LDA: Tune the number of topics, learning decay and batch size values\n",
    "    2. gensim LDA: Tune the number of topics, chunk size and passes values\n",
    "\n",
    "Parameters explanation:\n",
    "- **num_topics**: The number of topics to be extracted from the corpus.\n",
    "- **learning_decay**: The rate at which the learning rate decreases over time.\n",
    "- **batch_size**: The number of documents to use in each EM step.\n",
    "- **chunksize**: The number of documents to be used in each training chunk.\n",
    "- **passes**: The number of passes through the corpus during training."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ba74ccb949e795d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gianl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "nltk.download('stopwords')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T08:31:30.339301Z",
     "start_time": "2024-04-24T08:31:30.298301700Z"
    }
   },
   "id": "8c7d4f0f15fd2b06",
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "if you have incompatibility problems between gensim and scipy:\n",
    "- uninstall current version of scipy\n",
    "- run `pip install scipy==1.10.1`"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c2a42dc898e815b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load the dataset and preprocess the reviews"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6ccda62bbfa7bd3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(review: str) -> List[str]:\n",
    "    tokens = word_tokenize(review.lower())\n",
    "    # remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # keep only nouns\n",
    "    tokens = [word for word, pos in pos_tag(tokens) if pos.startswith('N')]\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T08:31:30.386299800Z",
     "start_time": "2024-04-24T08:31:30.308301Z"
    }
   },
   "id": "302e296dd75c49fc",
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": [
    "If the preprocessed reviews file does not exist, preprocess the reviews and save them to a file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a365396f7784cdd7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if 'reviews_preprocessed.txt' not in os.listdir('resources'):\n",
    "    # preprocess the reviews\n",
    "    df = pd.read_csv('resources/reviews.csv', nrows=3000)\n",
    "    reviews = df['Review']\n",
    "    reviews = [review.strip() for review in reviews] # remove newline characters from each review\n",
    "    # remove punctuation, stopwords, lemmatize and keep only nouns\n",
    "    reviews = [preprocess(review) for review in reviews]\n",
    "    # save the preprocessed reviews to a file\n",
    "    with open('resources/reviews_preprocessed.txt', 'w') as f:\n",
    "        for review in reviews:\n",
    "            f.write(','.join(review) + '\\n')\n",
    "else:\n",
    "    # load the preprocessed reviews\n",
    "    reviews = []\n",
    "    with open('resources/reviews_preprocessed.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            reviews.append(line.strip().split(','))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T08:31:30.416302600Z",
     "start_time": "2024-04-24T08:31:30.323303800Z"
    }
   },
   "id": "5dfc023a3d041928",
   "execution_count": 48
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the first review after preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe7de9e14b7ed3b2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['hotel',\n 'parking',\n 'deal',\n 'hotel',\n 'evening',\n 'review',\n 'valet',\n 'check',\n 'view',\n 'room',\n 'room',\n 'size',\n 'woke',\n 'pillow',\n 'soundproof',\n 'heard',\n 'music',\n 'room',\n 'night',\n 'morning',\n 'loud',\n 'bang',\n 'door',\n 'closing',\n 'people',\n 'neighbor',\n 'bath',\n 'product',\n 'stay',\n 'advantage',\n 'location',\n 'distance',\n 'experience',\n 'pay',\n 'parking',\n 'night']"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T08:31:30.422335700Z",
     "start_time": "2024-04-24T08:31:30.356301500Z"
    }
   },
   "id": "87a542a9923694e0",
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Topic Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b41a104e92539597"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1. sklearn LDA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "432d6ada7e32c58b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sklearn_lda_evaluate_models(reviews: List[List[str]], search_params: dict):\n",
    "    \"\"\"\n",
    "    Evaluate the LDA model using sklearn's implementation for different parameters settings.\n",
    "    :param reviews: list of preprocessed reviews\n",
    "    :param search_params: dictionary containing the parameters to be tuned and their possible values\n",
    "    :return: the best model found by the best combination of parameters\n",
    "    \"\"\"\n",
    "    reviews = [' '.join(review) for review in reviews]\n",
    "    \n",
    "    # convert the reviews to a term-document matrix\n",
    "    vectorizer = CountVectorizer()\n",
    "    data_vectorized = vectorizer.fit_transform(reviews)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(learning_method='online')\n",
    "    \n",
    "    # initiate GridSearchCV\n",
    "    model = GridSearchCV(lda, param_grid=search_params)\n",
    "    \n",
    "    # fit the GridSearchCV model\n",
    "    model.fit(data_vectorized)\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T08:31:30.425348Z",
     "start_time": "2024-04-24T08:31:30.371300800Z"
    }
   },
   "id": "f6cd6d8e44ac386d",
   "execution_count": 50
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2. gensim LDA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e75e333a9007c912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create a dictionary and a corpus from the preprocessed reviews as required by gensim's LDA model\n",
    "\n",
    "Dictionary:\n",
    "- The dictionary encapsulates the mapping between **normalized words** (nouns in this case) and their **integer ids**.\n",
    "- Each unique word is assigned a unique id.\n",
    "\n",
    "Corpus:\n",
    "- The corpus is a list of documents where each document is represented as a list of tuples.\n",
    "- Each tuple consists of a word's integer id and its frequency in the document.\n",
    "- This method converts each document (a list of words) into the bag-of-words format."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe2ab588a1ac4e15"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(reviews)\n",
    "\n",
    "corpus = [dictionary.doc2bow(text) for text in reviews]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T08:31:30.578302600Z",
     "start_time": "2024-04-24T08:31:30.388300700Z"
    }
   },
   "id": "6ec70fac018cf221"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print dictionary and corpus samples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7f19e6280a47a18"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Sample:\n",
      "ID 0: advantage\n",
      "ID 1: bang\n",
      "ID 2: bath\n",
      "ID 3: check\n",
      "ID 4: closing\n",
      "\n",
      "\n",
      "Corpus Sample:\n",
      "[('advantage', 1), ('bang', 1), ('bath', 1), ('check', 1), ('closing', 1), ('deal', 1), ('distance', 1), ('door', 1), ('evening', 1), ('experience', 1)]\n"
     ]
    }
   ],
   "source": [
    "# print dictionary sample\n",
    "print(\"Dictionary Sample:\")\n",
    "for i, (word_id, word) in enumerate(dictionary.iteritems()):\n",
    "    print(f\"ID {word_id}: {word}\")\n",
    "    if i == 4:\n",
    "        break\n",
    "        \n",
    "print(\"\\n\")\n",
    "\n",
    "# print the BoW representation for the first 3 documents in the corpus.\n",
    "print(\"Corpus Sample:\")\n",
    "# format output to show word counts along with their corresponding words\n",
    "formatted_doc = [(dictionary[word_id], count) for word_id, count in corpus[0]]\n",
    "print(formatted_doc[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T09:55:40.640732600Z",
     "start_time": "2024-04-24T09:55:40.621769Z"
    }
   },
   "id": "395a4408f890ebcb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluate the LDA model using gensim's implementation for different number of topics, chunksize and passes values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3203985955c2c07"
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [],
   "source": [
    "def gensim_lda_evaluate_models(corpus: List[List[str]],\n",
    "                    dictionary: corpora.Dictionary,\n",
    "                    texts: List[List[str]],\n",
    "                    topic_numbers: List[int],\n",
    "                    chunksize_values: List[int],\n",
    "                    passes_values: List[int],):\n",
    "    results = []\n",
    "    for num_topics in topic_numbers:\n",
    "        for chucksize_value in chunksize_values:\n",
    "            for passes_value in passes_values:\n",
    "    \n",
    "                print(\"Evaluating model with:\")\n",
    "                print(f\"num_topics={num_topics}, chucksize={chucksize_value}, passes={passes_value}\")\n",
    "                \n",
    "                model = gensim.models.ldamodel.LdaModel(\n",
    "                    corpus=corpus,\n",
    "                    id2word=dictionary,\n",
    "                    num_topics=num_topics,\n",
    "                    random_state=100,\n",
    "                    chunksize=chucksize_value,\n",
    "                    passes=passes_value,\n",
    "                    alpha=\"auto\",\n",
    "                    eta=\"auto\",\n",
    "                    per_word_topics=True\n",
    "                )\n",
    "                coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "                coherence_score = coherencemodel.get_coherence()\n",
    "                results.append((num_topics, chucksize_value, passes_value, coherence_score))\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T08:31:30.640300400Z",
     "start_time": "2024-04-24T08:31:30.596307Z"
    }
   },
   "id": "c4f4e37003ed67ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the model evaluation for sklearn LDA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f518320d04df994"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "sklearn_search_params = {'n_components': list(range(2, 16, 2)), 'learning_decay': [0.5, 0.7, 0.9], 'batch_size': [100, 200]}\n",
    "\n",
    "sklearn_results = sklearn_lda_evaluate_models(reviews, sklearn_search_params)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T09:09:54.993714100Z",
     "start_time": "2024-04-24T08:58:45.991708100Z"
    }
   },
   "id": "152db0b5a63903b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the top 5 models based on log likelihood score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bce7b6fd3ea1df29"
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Rank: 1\n",
      "Model's Params: {'batch_size': 100, 'learning_decay': 0.5, 'n_components': 2}\n",
      "Model's Log Likelihood Score: -215686.93851937214\n",
      "Model's Perplexity Score: 865.0991630931732\n",
      "\n",
      "\n",
      "Model Rank: 2\n",
      "Model's Params: {'batch_size': 100, 'learning_decay': 0.5, 'n_components': 4}\n",
      "Model's Log Likelihood Score: -222164.97607662837\n",
      "Model's Perplexity Score: 865.0991630931732\n",
      "\n",
      "\n",
      "Model Rank: 3\n",
      "Model's Params: {'batch_size': 100, 'learning_decay': 0.5, 'n_components': 6}\n",
      "Model's Log Likelihood Score: -228551.10209977944\n",
      "Model's Perplexity Score: 865.0991630931732\n",
      "\n",
      "\n",
      "Model Rank: 4\n",
      "Model's Params: {'batch_size': 100, 'learning_decay': 0.5, 'n_components': 8}\n",
      "Model's Log Likelihood Score: -234344.3129196315\n",
      "Model's Perplexity Score: 865.0991630931732\n",
      "\n",
      "\n",
      "Model Rank: 5\n",
      "Model's Params: {'batch_size': 100, 'learning_decay': 0.5, 'n_components': 10}\n",
      "Model's Log Likelihood Score: -240262.0742506627\n",
      "Model's Perplexity Score: 865.0991630931732\n",
      "\n",
      "\n",
      "Best Model's Params: {'batch_size': 200, 'learning_decay': 0.9, 'n_components': 2}\n",
      "Best Model's Log Likelihood Score: -211180.08898916066\n"
     ]
    }
   ],
   "source": [
    "temp_reviews = [' '.join(review) for review in reviews]\n",
    "\n",
    "# convert the reviews to a term-document matrix\n",
    "vectorizer = CountVectorizer()\n",
    "data_vectorized = vectorizer.fit_transform(temp_reviews)\n",
    "\n",
    "# print first 5 best models\n",
    "for i in range(5):    \n",
    "    print(f\"Model Rank: {i+1}\")\n",
    "    print(f\"Model's Params: {sklearn_results.cv_results_['params'][i]}\")\n",
    "    print(f\"Model's Log Likelihood Score: {sklearn_results.cv_results_['mean_test_score'][i]}\")\n",
    "    print(f\"Model's Perplexity Score: {sklearn_results.best_estimator_.perplexity(data_vectorized)}\")\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T09:42:13.179783700Z",
     "start_time": "2024-04-24T09:42:10.090128500Z"
    }
   },
   "id": "668ec58e44b64a83"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see the results in the file `sklearn_res_1.txt` in the results folder.\n",
    "\n",
    "A higher log likelihood score and a lower perplexity score indicate a better model.\n",
    "We can see that the best model has 2 topics, a learning decay of 0.5 and a batch size of 100.\n",
    "\n",
    "Note: perplexity might not be the best measure to evaluate topic models because it doesnâ€™t consider the context and semantic associations between words. A better measure is coherence score, as measured in the gensim LDA model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "40cf94c4be47d942"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the model evaluation for gensim LDA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c75ba598e2c78f9"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model with:\n",
      "num_topics=2, chucksize=100, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=2, chucksize=100, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=2, chucksize=100, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=2, chucksize=200, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=2, chucksize=200, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=2, chucksize=200, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=4, chucksize=100, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=4, chucksize=100, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=4, chucksize=100, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=4, chucksize=200, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=4, chucksize=200, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=4, chucksize=200, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=6, chucksize=100, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=6, chucksize=100, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=6, chucksize=100, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=6, chucksize=200, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=6, chucksize=200, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=6, chucksize=200, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=8, chucksize=100, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=8, chucksize=100, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=8, chucksize=100, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=8, chucksize=200, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=8, chucksize=200, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=8, chucksize=200, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=10, chucksize=100, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=10, chucksize=100, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=10, chucksize=100, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=10, chucksize=200, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=10, chucksize=200, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=10, chucksize=200, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=12, chucksize=100, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=12, chucksize=100, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=12, chucksize=100, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=12, chucksize=200, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=12, chucksize=200, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=12, chucksize=200, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=14, chucksize=100, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=14, chucksize=100, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=14, chucksize=100, passes=20\n",
      "Evaluating model with:\n",
      "num_topics=14, chucksize=200, passes=5\n",
      "Evaluating model with:\n",
      "num_topics=14, chucksize=200, passes=10\n",
      "Evaluating model with:\n",
      "num_topics=14, chucksize=200, passes=20\n",
      "Num Topics: 12, Chucksize: 200, Passes: 5, Coherence Score: 0.5404271700143645\n",
      "Num Topics: 12, Chucksize: 200, Passes: 20, Coherence Score: 0.5380960508582104\n",
      "Num Topics: 14, Chucksize: 200, Passes: 10, Coherence Score: 0.5341124664987279\n",
      "Num Topics: 12, Chucksize: 200, Passes: 10, Coherence Score: 0.5322717434467844\n",
      "Num Topics: 14, Chucksize: 200, Passes: 20, Coherence Score: 0.5321588549299386\n"
     ]
    }
   ],
   "source": [
    "topic_numbers = list(range(2, 16, 2))\n",
    "chunksize_values = [100, 200]\n",
    "passes_values = [5, 10, 20]\n",
    "\n",
    "# print coherence values to choose the best model\n",
    "results = gensim_lda_evaluate_models(corpus=corpus,\n",
    "                           dictionary=dictionary,\n",
    "                           texts=reviews,\n",
    "                           topic_numbers=topic_numbers,\n",
    "                           chunksize_values=chunksize_values,\n",
    "                           passes_values=passes_values)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T08:56:52.799577700Z",
     "start_time": "2024-04-24T08:42:08.029490900Z"
    }
   },
   "id": "608545ac5cd3c819"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the top 5 models based on coherence score"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "909a955fa6dcf946"
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Topics: 12, Chucksize: 200, Passes: 5, Coherence Score: 0.5404271700143645\n",
      "Num Topics: 12, Chucksize: 200, Passes: 20, Coherence Score: 0.5380960508582104\n",
      "Num Topics: 14, Chucksize: 200, Passes: 10, Coherence Score: 0.5341124664987279\n",
      "Num Topics: 12, Chucksize: 200, Passes: 10, Coherence Score: 0.5322717434467844\n",
      "Num Topics: 14, Chucksize: 200, Passes: 20, Coherence Score: 0.5321588549299386\n"
     ]
    }
   ],
   "source": [
    "results = sorted(results, key=lambda x: x[3], reverse=True)\n",
    "for num_topics, chucksize_value, passes_value, coherence_score in results[:5]:\n",
    "    print(f\"Num Topics: {num_topics}, Chucksize: {chucksize_value}, Passes: {passes_value}, Coherence Score: {coherence_score}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T08:58:22.447254Z",
     "start_time": "2024-04-24T08:58:22.385210200Z"
    }
   },
   "id": "e7076875ee9210d8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can see the results in the files `gensim_res_2.txt` and `gensim_res_1.txt` in the results folder.\n",
    "\n",
    "The best model, according to Gensim, has 12 topics, a chunk size of 100 and 5 passes.\n",
    "In this case we are considering the coherence score as the evaluation metric, which is a better measure than perplexity for the task of topic modeling. A higher coherence score indicates a better model."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d657f18712225e7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Show topics for the best Gensim model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "655128fe903a8a47"
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [],
   "source": [
    "best_gensim_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=dictionary,\n",
    "    num_topics=12,\n",
    "    random_state=100,\n",
    "    chunksize=100,\n",
    "    passes=5,\n",
    "    alpha=\"auto\",\n",
    "    eta=\"auto\",\n",
    "    per_word_topics=True\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T09:59:28.851488600Z",
     "start_time": "2024-04-24T09:59:24.378492700Z"
    }
   },
   "id": "547a9fa1a40723e1"
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.171*\"la\" + 0.084*\"none\" + 0.075*\"idea\" + 0.060*\"comment\" + 0.056*\"mean\" + 0.052*\"run\" + 0.017*\"range\" + 0.009*\"fountain\" + 0.008*\"flaw\" + 0.003*\"sleeping\"')\n",
      "\n",
      "\n",
      "(1, '0.112*\"orleans\" + 0.085*\"adult\" + 0.040*\"situation\" + 0.039*\"ceiling\" + 0.034*\"hurricane\" + 0.034*\"cold\" + 0.033*\"temperature\" + 0.033*\"conditioner\" + 0.022*\"odor\" + 0.020*\"period\"')\n",
      "\n",
      "\n",
      "(2, '0.097*\"cut\" + 0.009*\"shock\" + 0.000*\"chopin\" + 0.000*\"gaucho\" + 0.000*\"bracelet\" + 0.000*\"coco\" + 0.000*\"repellent\" + 0.000*\"topless\" + 0.000*\"restuarants\" + 0.000*\"ceremony\"')\n",
      "\n",
      "\n",
      "(3, '0.056*\"hotel\" + 0.030*\"restaurant\" + 0.024*\"place\" + 0.022*\"pool\" + 0.019*\"room\" + 0.017*\"lot\" + 0.017*\"area\" + 0.016*\"beach\" + 0.015*\"bar\" + 0.014*\"food\"')\n",
      "\n",
      "\n",
      "(4, '0.000*\"houer\" + 0.000*\"contribution\" + 0.000*\"keen\" + 0.000*\"offeringsuggestions\" + 0.000*\"caulk\" + 0.000*\"rusty\" + 0.000*\"approximity\" + 0.000*\"marshal\" + 0.000*\"proprietor\" + 0.000*\"danger\"')\n",
      "\n",
      "\n",
      "(5, '0.095*\"expectation\" + 0.081*\"reserve\" + 0.047*\"rude\" + 0.046*\"story\" + 0.037*\"dog\" + 0.034*\"mile\" + 0.029*\"checkout\" + 0.023*\"come\" + 0.023*\"fitness\" + 0.020*\"nightlife\"')\n",
      "\n",
      "\n",
      "(6, '0.107*\"language\" + 0.046*\"beware\" + 0.045*\"effort\" + 0.041*\"power\" + 0.038*\"sale\" + 0.036*\"accommodation\" + 0.034*\"meeting\" + 0.031*\"motel\" + 0.023*\"vehicle\" + 0.021*\"avoid\"')\n",
      "\n",
      "\n",
      "(7, '0.092*\"conference\" + 0.033*\"nyc\" + 0.030*\"edge\" + 0.027*\"october\" + 0.014*\"im\" + 0.011*\"shot\" + 0.011*\"length\" + 0.010*\"pub\" + 0.010*\"baseball\" + 0.009*\"deliver\"')\n",
      "\n",
      "\n",
      "(8, '0.113*\"villa\" + 0.065*\"refrigerator\" + 0.055*\"ok\" + 0.051*\"bargain\" + 0.016*\"gold\" + 0.000*\"freezer\" + 0.000*\"reunion\" + 0.000*\"condo\" + 0.000*\"rental\" + 0.000*\"past\"')\n",
      "\n",
      "\n",
      "(9, '0.048*\"hang\" + 0.021*\"friday\" + 0.013*\"tray\" + 0.000*\"flea\" + 0.000*\"volleyball\" + 0.000*\"wheelchair\" + 0.000*\"yr\" + 0.000*\"video\" + 0.000*\"polo\" + 0.000*\"g\"')\n",
      "\n",
      "\n",
      "(10, '0.057*\"room\" + 0.036*\"resort\" + 0.028*\"day\" + 0.027*\"time\" + 0.026*\"beach\" + 0.023*\"service\" + 0.020*\"food\" + 0.019*\"staff\" + 0.017*\"hotel\" + 0.017*\"night\"')\n",
      "\n",
      "\n",
      "(11, '0.073*\"agent\" + 0.072*\"credit\" + 0.056*\"card\" + 0.036*\"patio\" + 0.035*\"piece\" + 0.031*\"buck\" + 0.026*\"cloud\" + 0.024*\"conclusion\" + 0.023*\"police\" + 0.023*\"departure\"')\n"
     ]
    }
   ],
   "source": [
    "topics = best_gensim_model.print_topics(num_words=10)\n",
    "for topic in topics:\n",
    "    print(topic)\n",
    "    print(\"\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-24T10:02:12.899421900Z",
     "start_time": "2024-04-24T10:02:12.833377100Z"
    }
   },
   "id": "51d0373dac66b338"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
