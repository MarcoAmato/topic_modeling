{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Testing topic modeling on a tripadvisor reviews dataset\n",
    "\n",
    "1. Load the dataset and preprocess the reviews\n",
    "2. Topic Modeling using two different libraries:\n",
    "    1. sklearn LDA\n",
    "        1. Tune the number of topics and learning decay values\n",
    "    2. gensim LDA\n",
    "        1. Tune the number of topics, chunksize and passes values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ba74ccb949e795d"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gianl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import CoherenceModel\n",
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize, WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# if you have incompatibility problems between gensim and scipy:\n",
    "# - uninstall current version of scipy\n",
    "# - run `pip install scipy==1.10.1`"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T14:34:40.708966600Z",
     "start_time": "2024-04-12T14:34:37.861120500Z"
    }
   },
   "id": "8c7d4f0f15fd2b06",
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Load the dataset and preprocess the reviews"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e6ccda62bbfa7bd3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess(review: str) -> List[str]:\n",
    "    tokens = word_tokenize(review.lower())\n",
    "    # remove punctuation\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # remove stopwords\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    # lemmatize\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    # keep only nouns\n",
    "    tokens = [word for word, pos in pos_tag(tokens) if pos.startswith('N')]\n",
    "    return tokens"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T14:34:40.778054300Z",
     "start_time": "2024-04-12T14:34:40.704968900Z"
    }
   },
   "id": "302e296dd75c49fc",
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### If the preprocessed reviews file does not exist, preprocess the reviews and save them to a file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a365396f7784cdd7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "if 'reviews_preprocessed.txt' not in os.listdir('resources'):\n",
    "    # preprocess the reviews\n",
    "    df = pd.read_csv('resources/reviews.csv', nrows=3000)\n",
    "    reviews = df['Review']\n",
    "    reviews = [review.strip() for review in reviews] # remove newline characters from each review\n",
    "    # remove punctuation, stopwords, lemmatize and keep only nouns\n",
    "    reviews = [preprocess(review) for review in reviews]\n",
    "    # save the preprocessed reviews to a file\n",
    "    with open('resources/reviews_preprocessed.txt', 'w') as f:\n",
    "        for review in reviews:\n",
    "            f.write(','.join(review) + '\\n')\n",
    "else:\n",
    "    # load the preprocessed reviews\n",
    "    reviews = []\n",
    "    with open('resources/reviews_preprocessed.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            reviews.append(line.strip().split(','))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T14:47:21.224985600Z",
     "start_time": "2024-04-12T14:47:21.171194800Z"
    }
   },
   "id": "5dfc023a3d041928",
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print the first review after preprocessing"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe7de9e14b7ed3b2"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['hotel',\n 'parking',\n 'deal',\n 'hotel',\n 'evening',\n 'review',\n 'valet',\n 'check',\n 'view',\n 'room',\n 'room',\n 'size',\n 'woke',\n 'pillow',\n 'soundproof',\n 'heard',\n 'music',\n 'room',\n 'night',\n 'morning',\n 'loud',\n 'bang',\n 'door',\n 'closing',\n 'people',\n 'neighbor',\n 'bath',\n 'product',\n 'stay',\n 'advantage',\n 'location',\n 'distance',\n 'experience',\n 'pay',\n 'parking',\n 'night']"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T14:36:03.435277500Z",
     "start_time": "2024-04-12T14:36:03.420699500Z"
    }
   },
   "id": "87a542a9923694e0",
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Topic Modeling"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b41a104e92539597"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1. sklearn LDA\n",
    "\n",
    "Evaluate the LDA model using sklearn's implementation tuning the following parameters\n",
    "- Number of topics\n",
    "- Learning decay values\n",
    "\n",
    "Note: the search parameters can be different because "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "432d6ada7e32c58b"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Model's Params:  {'learning_decay': 0.9, 'n_components': 5}\n",
      "Best Log Likelihood Score:  -228915.35298277353\n",
      "Model Perplexity:  899.1034547729259\n"
     ]
    }
   ],
   "source": [
    "def sklearn_lda_evaluate_models(reviews: List[List[str]], search_params: dict):\n",
    "    reviews = [' '.join(review) for review in reviews]\n",
    "    \n",
    "    # convert the reviews to a document-term matrix\n",
    "    vectorizer = CountVectorizer()\n",
    "    data_vectorized = vectorizer.fit_transform(reviews)\n",
    "    \n",
    "    lda = LatentDirichletAllocation()\n",
    "    \n",
    "    # initiate GridSearchCV\n",
    "    model = GridSearchCV(lda, param_grid=search_params)\n",
    "    \n",
    "    # fit the GridSearchCV model\n",
    "    model.fit(data_vectorized)\n",
    "    \n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T14:42:12.805319300Z",
     "start_time": "2024-04-12T14:36:03.467274800Z"
    }
   },
   "id": "f6cd6d8e44ac386d",
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2. gensim LDA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e75e333a9007c912"
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Create a dictionary and a corpus from the preprocessed reviews as required by gensim's LDA model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe2ab588a1ac4e15"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create a dictionary from the processed reviews.\n",
    "# This dictionary encapsulates the mapping between normalized words (nouns in this case) and their integer ids.\n",
    "# Each unique word is assigned a unique id.\n",
    "dictionary = corpora.Dictionary(reviews)\n",
    "\n",
    "# Create a corpus from the processed reviews using the dictionary.\n",
    "# The corpus is a list of documents where each document is represented as a list of tuples.\n",
    "# Each tuple consists of a word's integer id and its frequency in the document.\n",
    "# This method converts each document (a list of words) into the bag-of-words format.\n",
    "corpus = [dictionary.doc2bow(text) for text in reviews]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T14:47:24.709232400Z",
     "start_time": "2024-04-12T14:47:24.534235Z"
    }
   },
   "id": "6ec70fac018cf221",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print dictionary and corpus samples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d7f19e6280a47a18"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary Sample:\n",
      "ID 0: advantage\n",
      "ID 1: bang\n",
      "ID 2: bath\n",
      "ID 3: check\n",
      "ID 4: closing\n",
      "ID 5: deal\n",
      "ID 6: distance\n",
      "ID 7: door\n",
      "ID 8: evening\n",
      "ID 9: experience\n",
      "\n",
      "\n",
      "Corpus Sample:\n",
      "[('advantage', 1), ('bang', 1), ('bath', 1), ('check', 1), ('closing', 1), ('deal', 1), ('distance', 1), ('door', 1), ('evening', 1), ('experience', 1)]\n"
     ]
    }
   ],
   "source": [
    "# print dictionary sample\n",
    "print(\"Dictionary Sample:\")\n",
    "for i, (word_id, word) in enumerate(dictionary.iteritems()):\n",
    "    print(f\"ID {word_id}: {word}\")\n",
    "    if i == 9:  # limit to the first 10 items\n",
    "        break\n",
    "        \n",
    "print(\"\\n\")\n",
    "\n",
    "# Print corpus sample\n",
    "# Print the BoW representation for the first 3 documents in the corpus.\n",
    "print(\"Corpus Sample:\")\n",
    "# Formatting output to show word counts along with their corresponding words\n",
    "formatted_doc = [(dictionary[word_id], count) for word_id, count in corpus[0]]\n",
    "print(formatted_doc[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T14:47:28.963677400Z",
     "start_time": "2024-04-12T14:47:28.896407600Z"
    }
   },
   "id": "395a4408f890ebcb",
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Evaluate the LDA model using gensim's implementation for different number of topics, chunksize and passes values"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b3203985955c2c07"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def gensim_lda_evaluate_models(corpus: List[List[str]],\n",
    "                    dictionary: corpora.Dictionary,\n",
    "                    texts: List[List[str]],\n",
    "                    topic_numbers: List[int],\n",
    "                    chunksize_values: List[int],\n",
    "                    passes_values: List[int],):\n",
    "    results = []\n",
    "    for num_topics in topic_numbers:\n",
    "        for chucksize_value in chunksize_values:\n",
    "            for passes_value in passes_values:\n",
    "    \n",
    "                print(\"Evaluating model with:\")\n",
    "                print(f\"num_topics={num_topics}, chucksize={chucksize_value}, passes={passes_value}\")\n",
    "                \n",
    "                model = gensim.models.ldamodel.LdaModel(\n",
    "                    corpus=corpus,\n",
    "                    id2word=dictionary,\n",
    "                    num_topics=num_topics,\n",
    "                    random_state=100,\n",
    "                    chunksize=chucksize_value,\n",
    "                    passes=passes_value,\n",
    "                    alpha=\"auto\",\n",
    "                    eta=\"auto\",\n",
    "                    per_word_topics=True\n",
    "                )\n",
    "                coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "                coherence_score = coherencemodel.get_coherence()\n",
    "                results.append((num_topics, chucksize_value, passes_value, coherence_score))\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-12T14:47:39.968947900Z",
     "start_time": "2024-04-12T14:47:39.939947700Z"
    }
   },
   "id": "c4f4e37003ed67ba",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the model evaluation for sklearn LDA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f518320d04df994"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sklearn_search_params = {'n_components': [5, 10, 15], 'learning_decay': [0.5, 0.7, 0.9]}\n",
    "\n",
    "sklearn_lda_evaluate_models(reviews, sklearn_search_params)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "152db0b5a63903b5"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Run the model evaluation for gensim LDA"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c75ba598e2c78f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "topic_numbers = list(range(2, 15))\n",
    "chunksize_values = [50, 100, 200]\n",
    "passes_values = [5, 10, 20]\n",
    "\n",
    "# Print coherence values to choose the best model\n",
    "results = gensim_lda_evaluate_models(corpus=corpus,\n",
    "                           dictionary=dictionary,\n",
    "                           texts=reviews,\n",
    "                           topic_numbers=topic_numbers,\n",
    "                           chunksize_values=chunksize_values,\n",
    "                           passes_values=passes_values)\n",
    "\n",
    "# Sort results by coherence score and print the top 5 models\n",
    "results = sorted(results, key=lambda x: x[3], reverse=True)\n",
    "for num_topics, chucksize_value, passes_value, coherence_score in results[:5]:\n",
    "    print(f\"Num Topics: {num_topics}, Chucksize: {chucksize_value}, Passes: {passes_value}, Coherence Score: {coherence_score}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "608545ac5cd3c819"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
